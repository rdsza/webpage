---
title: 'Maximum likelihood estimation'
date: 2022-01-01
permalink: /posts/2022/01/MLE/
tags:
  - machine learning
---

## Maximum Likelihood Estimation

Maximum likelihood estimation (MLE) is a fundamental method for estimating the parameters of a probability distribution based on observed data. The core idea is to find the parameter values that make the observed data most probable under the assumed model.

## The Likelihood Function

Given observed data $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ assumed to be drawn i.i.d. from a distribution $p(x \mid \theta)$, the **likelihood function** is:

$$L(\theta \mid \mathbf{x}) = \prod_{i=1}^{n} p(x_i \mid \theta)$$

The MLE is the value of $\theta$ that maximizes this function:

$$\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \, L(\theta \mid \mathbf{x})$$

## The Log-Likelihood

In practice, we work with the **log-likelihood** because products become sums, which are numerically more stable and easier to differentiate:

$$\ell(\theta \mid \mathbf{x}) = \ln L(\theta \mid \mathbf{x}) = \sum_{i=1}^{n} \ln p(x_i \mid \theta)$$

The MLE is found by solving the **score equation**:

$$\frac{\partial \ell(\theta \mid \mathbf{x})}{\partial \theta} = 0$$

and verifying that the solution corresponds to a maximum (via the second derivative or Hessian).

## Example: Gaussian Distribution

Suppose $x_1, \ldots, x_n \sim \mathcal{N}(\mu, \sigma^2)$. The log-likelihood is:

$$\ell(\mu, \sigma^2) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i - \mu)^2$$

Setting the partial derivatives to zero yields the familiar results:

$$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n} x_i, \qquad \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i - \hat{\mu})^2$$

Note that the MLE for $\sigma^2$ is biased (divides by $n$ rather than $n-1$), but it is consistent and asymptotically efficient.

## Properties of MLE

Under regularity conditions, the MLE has desirable asymptotic properties:

- **Consistency**: $\hat{\theta}_{\text{MLE}} \xrightarrow{p} \theta_0$ as $n \to \infty$.
- **Asymptotic normality**: $\sqrt{n}(\hat{\theta}_{\text{MLE}} - \theta_0) \xrightarrow{d} \mathcal{N}(0, I(\theta_0)^{-1})$, where $I(\theta_0)$ is the Fisher information.
- **Asymptotic efficiency**: The MLE achieves the Cram√©r-Rao lower bound asymptotically.
- **Invariance**: If $\hat{\theta}$ is the MLE of $\theta$, then $g(\hat{\theta})$ is the MLE of $g(\theta)$ for any function $g$.

## Limitations

- MLE may not exist for small samples or degenerate cases.
- The solution may not be unique (multimodal likelihood).
- MLE can overfit with limited data, as it has no built-in regularization (unlike Bayesian approaches with priors).
- MLE assumes the model family is correctly specified; if the true distribution is not in the parametric family, MLE converges to the closest distribution in the KL-divergence sense.

## Connection to Machine Learning

Many common objectives in machine learning are instances of MLE:

- **Logistic regression**: Maximizing the likelihood of a Bernoulli model parameterized by a linear function.
- **Neural network training**: Minimizing cross-entropy loss is equivalent to maximizing the log-likelihood under a categorical distribution.
- **Gaussian mixture models**: The EM algorithm iteratively maximizes the likelihood for mixture models.

MLE provides a principled foundation for parameter estimation and connects directly to information-theoretic concepts like KL divergence and cross-entropy.
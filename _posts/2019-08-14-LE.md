---
title: 'Manifold Learning: Laplacian Eigenmaps'
date: 2019-08-14
permalink: /posts/2019/08/LE/
tags:
  - machine learning
---

Laplacian Eigenmaps is a nonlinear dimensionality reduction technique that uses the spectral properties of the graph Laplacian to embed high-dimensional data into a lower-dimensional space while preserving local neighborhood structure.

## Motivation

Many real-world datasets lie on or near a low-dimensional manifold embedded in a high-dimensional ambient space. Linear methods like PCA capture global variance but can fail to uncover the intrinsic nonlinear structure. Laplacian Eigenmaps, introduced by Belkin and Niyogi (2003), explicitly accounts for the geometry of the data manifold.

## The Algorithm

The Laplacian Eigenmaps algorithm proceeds in three steps:

### Step 1: Construct the Neighborhood Graph

Build a graph $G = (V, E)$ where each data point $x_i$ is a node. Connect $x_i$ and $x_j$ with an edge if:

- $x_j$ is among the $k$-nearest neighbors of $x_i$ (k-NN graph), or
- $\|x_i - x_j\| < \epsilon$ for some threshold $\epsilon$ ($\epsilon$-neighborhood graph).

### Step 2: Compute Edge Weights

Assign weights to the edges using a heat kernel:

$$W_{ij} = \exp\left(-\frac{\|x_i - x_j\|^2}{t}\right)$$

where $t$ is a bandwidth parameter. Alternatively, one can use simple binary weights ($W_{ij} = 1$ if connected, $0$ otherwise).

### Step 3: Solve the Generalized Eigenproblem

Compute the graph Laplacian $L = D - W$, where $D$ is the diagonal degree matrix with $D_{ii} = \sum_j W_{ij}$. The embedding is obtained by solving the generalized eigenvalue problem:

$$L f = \lambda D f$$

The eigenvectors corresponding to the smallest non-zero eigenvalues provide the coordinates of the low-dimensional embedding.

## Why Does This Work?

The objective function minimized by Laplacian Eigenmaps is:

$$\min_f \sum_{i,j} W_{ij} \|f(x_i) - f(x_j)\|^2 = f^T L f$$

subject to $f^T D f = 1$. This penalizes mappings that place nearby points (those with large $W_{ij}$) far apart in the embedding space. Points that are neighbors in the original space remain close in the low-dimensional representation.

## Connection to the Laplace-Beltrami Operator

In the continuous limit (as the number of data points $n \to \infty$ and the neighborhood size shrinks appropriately), the graph Laplacian converges to the **Laplace-Beltrami operator** on the underlying manifold. The eigenfunctions of this operator provide a natural coordinate system for the manifold, analogous to Fourier harmonics on a flat space.

## Practical Considerations

- **Choice of $k$ or $\epsilon$**: Too small and the graph becomes disconnected; too large and local structure is lost.
- **Number of dimensions**: Typically chosen by examining the eigenvalue spectrum â€” a spectral gap suggests the appropriate dimensionality.
- **Scalability**: The algorithm requires computing a sparse eigendecomposition, which scales well for large datasets using iterative solvers.

## Applications

Laplacian Eigenmaps has been widely applied in:

- **Image and speech processing**: Discovering low-dimensional representations for recognition tasks.
- **Structural biology**: Extracting conformational variability from cryo-EM datasets, where each image is a noisy 2D projection of a 3D molecule in a different conformation.
- **Clustering and visualization**: Embedding data for visual exploration and downstream clustering.


